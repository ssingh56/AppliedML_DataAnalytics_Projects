{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to analyze employee_attrition.csv dataset provided. The dataset provides a variety of information about the employees, such as demographics, time on job, etc. and also if they will stay with or leave the company('Attrition' attribute 1(Yes)/0(No))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R Shiny App: https://sankalpsingh.shinyapps.io/HW01/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "import apyori as ap\n",
    "from apyori import apriori #Apriori Algorithm\n",
    "import mlxtend as ml\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('employee_attrition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing & EDA\n",
    "#### I have followed the CRISP-DM methodology here which starts with data understanding and then data cleaning & pre-processing. I have addressed the following data issues here: \n",
    "1) Find missing values and impute them accordingly\n",
    "\n",
    "2) Remove irrelevant columns \n",
    "\n",
    "3) Handling outliers\n",
    "\n",
    "4) Removing highly correlated columns\n",
    "\n",
    "5) Discretization of continuous attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Find missing values and impute them accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As Gender and Over18 are categorical nominal attributes, we replace the missing values in them with mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Gender\", data=df).set_title('Distribution based on Gender for our dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'].fillna('Male', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OverTime'].fillna('No', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below is a correlation matrix plot - this will help us impute missing values and would also help us later to remove highly correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.title(\"Correltaion Matrix\");\n",
    "sns.heatmap(df.corr(),annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DistanceFromHome: We use the median value as the data exhibits skewness. We can safely do this because the number of missing values is less. So, it won't affect the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DistanceFromHome.fillna(np.nanmedian(df.DistanceFromHome),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will be replacing the missing value in JobLevel, PercentSalaryHike by median as we can see from the below plot that the data is right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['JobLevel'],kde = False).set_title('Distribution for JobLevel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.JobLevel.fillna(np.nanmedian(df.JobLevel),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['PercentSalaryHike'],kde = False).set_title('Distribution for PercentSalaryHike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.PercentSalaryHike.fillna(np.nanmedian(df.PercentSalaryHike),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will be replacing the missing value in RelationshipSatisfaction by mean (integer closest to mean) as we can see from the below plot that the data is left skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['RelationshipSatisfaction'],kde = False).set_title('Distribution for RelationshipSatisfaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RelationshipSatisfaction'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.RelationshipSatisfaction.fillna(3,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As PerformanceRating attribute just contains 2 unique values, we can easily replace the missing value with the mode here i.e. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['PerformanceRating'],kde = False).set_title('Distribution for PerformanceRating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.PerformanceRating.fillna(3,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will be replacing the missing value in TotalWorkingYears, YearsSInceLastPromotion by median as we can see from the below plot that the data is right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['TotalWorkingYears'],kde = False).set_title('Distribution for TotalWorkingYears')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TotalWorkingYears.fillna(np.nanmedian(df.TotalWorkingYears),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['YearsSinceLastPromotion'],kde = False).set_title('Distribution for YearsSinceLastPromotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.YearsSinceLastPromotion.fillna(np.nanmedian(df.YearsSinceLastPromotion),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of missing values in the dataframe after cleaning:\",df.isna().sum().sum()) #We check if there are any missing values in our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Removing irrelevant columns -  We have removed the below columns as all of these contain either one unique value / all unique values. So, these attributes won't help us improve our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['EmployeeCount']\n",
    "del df['EmployeeNumber']\n",
    "del df['StandardHours']\n",
    "del df['Over18']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Handling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the plot below, we can see that we have an outlier in DistanceFromHome attribute. It could be a possibility that the person is working remotely so the distance from home is large. We will categorize this datapoint into high_DistanceFromHome later while binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"DistanceFromHome\", data=df)\n",
    "plt.title(\"Box plot DistanceFromHome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the plot below, we can see that we have an outlier in TotalWorkingYears attribute. The outlier value seems incorrect as the total working years is more than 100 years. We replace this data point with the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"TotalWorkingYears\", data=df)\n",
    "plt.title('Box Plot TotalWorkingYears')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TotalWorkingYears[143]=14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Removing highly correlated columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Due to high correlation between Monthly Income and Job Level, we will drop the Monthly Income column out of the two columns. As Job Level is already a categorical variable, so we won't need to do any preprocessing in order to use in the association rule model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"MonthlyIncome\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Data Transformations: Discretization - As Association rules mining only takes categorical attributes as input, so now we will be discretizing all the continuous numerical variables into discrete variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the below plot we can see that Age is normally distributed. So, we will discretize it into 3 labels - Low, Medium, High Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Age'],kde = False).set_title('Distribution for Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"] = pd.qcut(df.Age, 3, labels = ['low_age','med_age','high_age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similarly, we will be discretizing all the continuos numerical attributes into bins accordingly - Low, Medium, High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['DailyRate'],kde = False).set_title('Distribution for DailyRate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DailyRate\"] = pd.cut(df.DailyRate, 3, labels = ['low_DailyRate','med_DailyRate','high_DailyRate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['DistanceFromHome'],kde = False).set_title('Distribution for DistanceFromHome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DistanceFromHome\"] = pd.cut(df.DistanceFromHome, 3, labels = ['low_DistanceFromHome','med_DistanceFromHome','high_DistanceFromHome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['HourlyRate'],kde = False).set_title('Distribution for HourlyRate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HourlyRate\"] = pd.cut(df.HourlyRate, 3, labels = ['low_HourlyRate','med_HourlyRate','high_HourlyRate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['MonthlyRate'],kde = False).set_title('Distribution for MonthlyRate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MonthlyRate\"] = pd.cut(df.MonthlyRate, 3, labels = ['low_MonthlyRate','med_MonthlyRate','high_MonthlyRate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discretizing all the other continuous numerical variables similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"NumCompaniesWorked\"] = pd.cut(df.NumCompaniesWorked, 3, labels = ['low_NumCompaniesWorked','med_NumCompaniesWorked','high_NumCompaniesWorked'])\n",
    "\n",
    "df[\"PercentSalaryHike\"] = pd.cut(df.PercentSalaryHike, 3, labels = ['low_PercentSalaryHike','med_PercentSalaryHike','high_PercentSalaryHike'])\n",
    "\n",
    "df[\"TotalWorkingYears\"] = pd.cut(df.TotalWorkingYears, 3, labels = ['low_TotalWorkingYears','med_TotalWorkingYears','high_TotalWorkingYears'])\n",
    "\n",
    "df[\"TrainingTimesLastYear\"] = pd.cut(df.TrainingTimesLastYear, 3, labels = ['low_TrainingTimesLastYear','med_TrainingTimesLastYear','high_TrainingTimesLastYear'])\n",
    "\n",
    "df[\"YearsAtCompany\"] = pd.cut(df.YearsAtCompany, 3, labels = ['low_YearsAtCompany','med_YearsAtCompany','high_YearsAtCompany'])\n",
    "\n",
    "df[\"YearsInCurrentRole\"] = pd.cut(df.YearsInCurrentRole, 3, labels = ['low_YearsInCurrentRole','med_YearsInCurrentRole','high_YearsInCurrentRole'])\n",
    "\n",
    "df[\"YearsSinceLastPromotion\"] = pd.cut(df.YearsSinceLastPromotion, 3, labels = ['low_YearsSinceLastPromotion','med_YearsSinceLastPromotion','high_YearsSinceLastPromotion'])\n",
    "\n",
    "df[\"YearsWithCurrManager\"] = pd.cut(df.YearsWithCurrManager, 3, labels = ['low_YearsWithCurrManager','med_YearsWithCurrManager','high_YearsWithCurrManager'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more visualizations and insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"BusinessTravel\", data=df).set_title('Distribution for Business Travel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Department\", data=df).set_title('Distribution for Department')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below plot generates an insight that people who have lower environmental satisfaction tend to quit their jobs more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='Attrition', x='EnvironmentSatisfaction', data=df)\n",
    "plt.title('Attrition vs Environment Satisfaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below plot shows that people in the lower age groups tend to quit their jobs more as compared to their senior counterparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(15,5))\n",
    "df_plot = df.groupby([\"Attrition\", \"Age\"]).size().reset_index().pivot(columns=\"Attrition\", index=\"Age\", values=0)\n",
    "sns.countplot(df[\"Age\"],ax=ax[0]).set_title(\"Countplot of Age\");\n",
    "df_plot.div(df_plot.sum(axis=1), axis=0).plot(kind='bar', stacked=True, ax=ax[1]);\n",
    "ax[1].set_title(\"Stacked Proportion Plot of Attrition based on Age\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - ARules Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing dataframe for Association rules mining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the df columns to string\n",
    "df=df.astype(\"str\")\n",
    "\n",
    "# converting all the columns to object type to fulfill the type requirement of an association rules\n",
    "df=df.astype(\"object\")\n",
    "\n",
    "#Final dataset to be used for R Shiny App\n",
    "df.to_csv(\"ShinyApp_data.csv\", index=None)\n",
    "\n",
    "# Creating a new dataframe for arules mining\n",
    "df2 = pd.DataFrame({col: str(col)+'=' for col in df}, \n",
    "                index=df.index) + df.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i in range(0,len(df)):\n",
    "    records.append([str(df2.values[i,j]) \n",
    "    for j in range(0, len(df2.columns))])\n",
    "frequent_itemset = ap.apriori(records, min_support=0.5, min_confidence=0.5,\n",
    "                              min_lift=1,min_length=2)\n",
    "results = list(frequent_itemset)\n",
    "len(results)\n",
    "results[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we have run a baseline model, we list all the results by descending order of support. Most of the antecedents in the rules are of length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records)\n",
    "df3 = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "frequent_itemsets = apriori(df3, min_support=0.5, use_colnames=True)\n",
    "frequent_itemsets.sort_values(by='support',ascending=False).head(10)\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "rules.head()\n",
    "rules[(rules['lift']>1) & (rules['confidence'] > 0.5)].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SupervisedApriori(data,consequent,min_supp,min_conf,min_lift):\n",
    "    frequent_itemsets = apriori(data, min_supp, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_conf)\n",
    "    #filter according to lift\n",
    "    rules = rules[rules['lift'] > min_lift]\n",
    "    sup_rules = pd.DataFrame()\n",
    "    for i in consequent:\n",
    "        df3 = rules[rules['consequents'] == {i}]\n",
    "        sup_rules = sup_rules.append(df3,ignore_index = True)\n",
    "    return(sup_rules)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attrition=No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we run the model to predict rules for consequent Attrition as No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_no = SupervisedApriori(df3,consequent = ['Attrition=No'],\n",
    "min_supp=0.4, min_conf=0.9, min_lift=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_no.sort_values(\"lift\",ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in attrition_no.sort_values(\"lift\",ascending=False).head().index:\n",
    "    print(attrition_no.loc[i][\"antecedents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attrition=Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we run the model to predict rules for consequent Attrition as Yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_yes = SupervisedApriori(df3,consequent = ['Attrition=Yes'],\n",
    "min_supp=0.04, min_conf=0.3, min_lift=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_yes.sort_values(\"lift\",ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in attrition_yes.sort_values(\"lift\",ascending=False).head().index:\n",
    "    print(attrition_yes.loc[i][\"antecedents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attrition=No:  All the rules have a lift greater than 1 so we can say that there is some relationship between the antecedents and the consequents.  Confidence is very high meaning that our rules are very significant. From the rules, we can generate a few insights: people in the R&D department tend to stay moreloyal to their jobs; less attrition rate is predicted when employees are not told to do over time; low distance from home and low years worked under current manager are also factors that can lead to employee retainment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attrition=Yes: Rules have a high lift of 3 and the confidence is above .45. So, the rules are not caused due to randomness. One factor leading to employee attrition is - employee who have no stock option tend to leave the comapany. Other factors that can lead to employee attrition are- TotalWorkingYears= low, Overtime=Yes, Marital Status=Yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
